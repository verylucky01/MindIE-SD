> 本章节主要介绍`以存代算`相关加速特性
> - [DitCache](#ditcache)
> - [AttentionCache](#attentioncache)



## DitCache
- 背景：DiT模型在推理过程中会循环迭代T个步骤，每个步骤t都会完整的计算所有的block，而每个block都包含大量的计算操作（如图所示）。但相邻步骤之间的latent很相似，导致推理过程中反复计算几乎相同的中间结果，造成计算冗余，推理速度慢。

![](../figures/ditcache-image-1.png)

- 原理：基于相邻迭代采样步骤间、或相邻block间的激活相似性，复用模型局部特征，跳过指定的DiTBlock，减少冗余计算，实现模型推理加速。
- 优化方法：通过搜索脚本，根据加速比计算需要跳过的最少block数，然后从起止block开始遍历，在所有可能的组合中，找到MSE损失最小的配置作为最优解。其核心优化点在于：当缓存命中时，直接复用stepN中特定block区间的计算结果到stepM，从而将整个DiTBlock序列的正向传播计算变成一次简单的张量读取操作。

    ![](../figures/ditcache-image-2.png)
  - step1：根据加速比计算最小需要cache的block数； 
  - step2：在每个step中，由于block0需要计算，所以从block1开始遍历，通过计算找到最小的三组block start和block end； 
  - step3：遍历step2得到的所有可能的组合，计算模型cache前和cache后的MSE，找到MSE损失最小的配置作为最优解； 
  - step4：将上述步骤得到的参数配置在模型中，并在执行推理时开启cache完成加速。

- 优化流程：
  - 调用CacheConfig和CacheAgent接口。
    ```python
    from mindiesd import CacheConfig, CacheAgent
    ```
  - 初始化CacheConfig。
    ```python
    config = CacheConfig(
            method="dit_block_cache",
            blocks_count=len(transformer.single_blocks), # 使能cache的block的个数
            steps_count=args.infer_steps,                # 模型推理的总迭代步数
            step_start=args.cache_start_steps,           # 开始进行cache的步数索引
            step_interval=args.cache_interval,           # 强制重新计算的间隔步数
            step_end=args.infer_steps-1,                 # 停止cache的步数索引
            block_start=args.single_block_start,         # 每一步中，开始进行cache的block索引
            block_end=args.single_block_end              # 每一步中，停止cache的block索引
        )
    ```
  - 初始化CacheAgent并赋值给block。
    ```python
    cache_agent = CacheAgent(config)
    # 对dit block粒度进行cache
    hunyuan_video_sampler.pipeline.transformer.cache = cache_agent
    ```
  - 使能cache进行推理
    ```python
    x = self.cache.apply(block,
                         hidden_states=x,
                         vec=vec,
                         txt_len=txt_seq_len,
                         cu_seqlens_q=cu_seqlens_q,
                         cu_seqlens_kv=cu_seqlens_kv,
                         max_seqlen_q=max_seqlen_q,
                         max_seqlen_kv=max_seqlen_kv,
                         freqs_cis=freqs_cis)
    ```

- 示例参考: [DiTCache加速特性使用样例](../examples/DiTCache加速特性使用样例.md)


---
## AttentionCache 

- 背景：模型在推理过程中会循环迭代T个步骤，每个步骤中包含多个block，而每个block都包含大量的计算操作，比如STA（如图所示）。但相邻步骤之间的block中的attention层比较相似，导致推理过程中反复计算几乎相同的中间结果，造成计算冗余，推理速度慢。

![](../figures/attentioncache-image-1.png)

- 原理：基于相邻时间步的特性相似性，与DitCache不同，AttentionCache通过复用block里的Attention计算结果，从而跳过部分Attention层，减少冗余计算，实现模型推理加速。
- 优化方法：通过搜索脚本，根据加速比计算需要跳过的最小Attention次数，然后遍历起止step，在所有可能的组合中，找到MSE损失最小的配置作为最优解。其核心优化点在于：利用以空间换时间的原理，直接复用stepN的block的Attention层的计算缓存结果到stepM，显著减少Attention层的计算量。

    ![](../figures/attentioncache-image-2.png)
  - step1：根据加速比ratio计算最小需要跳过的attention次数；
  - step2：根据开始step和min_skip_attention计算出min_interval与step_end，并遍历所有可能的结果，计算模型cache前和cache后的MSE，找到MSE损失最小的配置作为最优解；
  - step3：将上述步骤得到的参数配置在模型中，并在执行推理时开启cache完成加速。

- 优化流程：
  - 调用CacheConfig和CacheAgent接口。
    ```python
    from mindiesd import CacheConfig, CacheAgent
    ```
  - 初始化CacheConfig，对于attention\_cache、block\_start和block\_end，可采用默认值。
    ```python
    config = CacheConfig(
                method="attention_cache",
                blocks_count=len(transformer.single_blocks), # 使能cache的block的个数
                steps_count=args.infer_steps,                # 模型推理的总迭代步数
                step_start=args.start_step,                  # 开始进行cache的步数索引
                step_interval=args.attentioncache_interval,  # 强制重新计算的间隔步数
                step_end=args.end_step                       # 停止cache的步数索引
            )
    ```
  - 初始化CacheAgent并赋值给block。
    ```python
    cache_agent = CacheAgent(config)
    # 对block里的attention部分进行cache
    for block in transformer.single_blocks:
        block.cache = cache_agent
    ```
  - 使能cache进行推理
    ```python
    attn = self.cache.apply(self.double_forward,
                                img=img, txt=txt,
                                img_mod1_shift=img_mod1_shift,
                                img_mod1_scale=img_mod1_scale,
                                txt_mod1_shift=txt_mod1_shift,
                                txt_mod1_scale=txt_mod1_scale,
                                freqs_cis=freqs_cis,
                                cu_seqlens_q=cu_seqlens_q,
                                cu_seqlens_kv=cu_seqlens_kv,
                                max_seqlen_q=max_seqlen_q,
                                max_seqlen_kv=max_seqlen_kv)
    ```

- 示例参考: [AttentionCache加速特性使用样例](../examples/AttentionCache加速特性使用样例.md)
