> 本章节主要介绍`显存优化`相关加速特性
> - [张量并行](#张量并行)
> - [显存共享](#显存共享)
> - [异步offload](#异步offload)
> - [量化](#量化)

## 张量并行
参考多卡并行章节：[张量并行](./multi-device_parallelism.md#张量并行)

---
## 显存共享
- 核心问题：多实例场景下，在同一个NPU设备多个模型使用了相同的权重（如图所示），可使用显存共享降低消耗。

    ![](../figures/%E6%98%BE%E5%AD%98%E5%85%B1%E4%BA%AB-image-1.png)
- 理论支撑：利用相同的NPU物理地址和偏移构建不同Tensor，可同时访问同一片内存。 
- 设计思路：使用进程间共享的内存管理器管理内存，不同进程使用内存管理器分配的内存进行共享。 
- 实现流程：

    ![](../figures/%E6%98%BE%E5%AD%98%E5%85%B1%E4%BA%AB-image-2.png)
  - 进程0统计所需的内存大小offset，通过进程间共享的NPU Allocator申请内存；
  - NPU Allocator将申请的物理内存地址data_ptr返回给进程0；
  - 进程0将实际物理内存地址data_ptr通过进程间通信传给进程1； 
  - 进程0触发内存拷贝，将CPU的内存拷贝到实际NPU物理地址上； 
  - 进程0和进程1通过物理内存地址data_ptr和offset构建Tensor。

---
## 异步offload
- 背景：在同步offload时，GPU计算完一层后需要停止，等待将第二层的权重从CPU搬运到GPU，权重加载完成后再继续计算，这样会造成GPU大部分时间都在空闲等待，利用率降低。
- 优化方法：
  - 异步Offload加载是一种用时间（推理速度）换空间（显存容量）的典型技术。
  - 其原理为：通过异步流水线设计，同时进行计算和权重加载，使得GPU在计算第一层时，第二层的权重已经开始搬运，当第一层计算完成时，第二层的权重也已经加载完成，这样可以用计算耗时掩盖掉搬运耗时，使得GPU空闲时间减少，整体推理速度显著提升。
- 下图分别展示了offload流程和异步offload流程

    ![](../figures/offload%E6%B5%81%E7%A8%8B-image.png)    ![](../figures/%E5%BC%82%E6%AD%A5offload-image.png)

## 量化
参考轻量化算法章节：[Linear量化](./lightweight_algorithm.md#linear量化)